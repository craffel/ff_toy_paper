\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems}


\author{
Colin Raffel
LabROSA
Columbia University
New York, NY 10027
\texttt{craffel@gmail.com}
\And
Daniel P.~W.~Ellis
LabROSA
Columbia University
New York, NY 10027
\texttt{dpwe@ee.columbia.edu}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Recently, recurrent neural networks (RNNs) have been augmented with ``attention'' mechanisms which compute a fixed-length representation of entire sequences.
We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that it can solve some long-term memory problems (specifically, those where temporal order doesn't matter).
In fact, we show empirically that our model can solve these problems for sequence lengths which are both longer and more widely varying than has been shown for RNNs.
\end{abstract}

\section{Attention}

\end{document}
